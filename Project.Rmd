---
title: "STAT 5125 Final Project"
author: "Josh Moak and Owen Fiore"
date: "2024-04-22"
output: html_document
---

In this project we seek to explore patterns and identify trends in a dataset
consisting of 10,000 car insurance claims available at [the link here](https://www.kaggle.com/datasets/sagnik1511/car-insurance-data). You can also find the data available in the repo at `Car_Insurance_Claim.csv`.

# Data Import
```{r}
df <- read.csv("Car_Insurance_Claim.csv")
df
```

We read in the data and we see that some data is character, some is numeric, and some is stored in a dbl. 
```{}
Regex: AGE, driving experience, vehicle year
```

# Data Cleaning
### Impute missing values
The first thing we are going to do is deal with missing data.
```{r}
library(tidyverse)
library(visdat)
df |> vis_dat()
```
Here we see that we have a lot of data of type character which will likely need to be changed.  However what we see is we only have 2 columns with missing data which is good.
```{r}
df |> vis_miss()
```
```{r}
library(naniar)
df |> miss_var_summary()
```

Lets impute missing values based on the median of the column
```{r}
library(naniar)
df <- df |>
  nabular(only_miss = TRUE) |>
  mutate(CREDIT_SCORE = naniar::impute_median(CREDIT_SCORE),
         ANNUAL_MILEAGE = naniar::impute_median(ANNUAL_MILEAGE)) |>
  select(-CREDIT_SCORE_NA, -ANNUAL_MILEAGE_NA)
```

```{r}
df |> n_miss()
```
### Regular Expression 
We see there are no longer any missing values.  We are now ready to use regular expression to alter and extract the meaningful string data.  We have 3 columns of interest for this part: `AGE`, `Driving_Experince`, and `VEHICLE_YEAR`
```{r}
df |> select(AGE) |> slice_head(n = 10)
```
Using this and our knowledge of who drives cars, we know that all the information we need is going to be contained in the first two integers as somebody who is less than 10 years old cannot drive and anybody who would be 100 is grouped into 65+. Thus we need a regular expression to keep only the first 2 numbers.

```{r}
df <- df |>
  mutate(AGE = as.integer(stringr::str_extract(AGE, "^\\d{1,2}")))
df |> select(AGE) |> slice_head(n = 10)
```
This looks much better and is something that can be converted to a factor variable later on.

```{r}
df |> select(DRIVING_EXPERIENCE) |> slice_head(n = 10)
```
Lets use the `parse_number` function from the `readr` package to extract the first number available.
```{r}
df <- df |>
  mutate(DRIVING_EXPERIENCE = parse_number(DRIVING_EXPERIENCE))
df |> select(DRIVING_EXPERIENCE) |> slice_head(n = 10)
```
This looks much better.

Lastly we look at `VEHICLE_YEAR`.
```{r}
df |> select(VEHICLE_YEAR) |> slice_head(n = 10)
```
There seem to be only two types of data in this column: `before 2015` and `after 2015`. We are going to make this easier for us later on and convert this data to numeric.
```{r}
library(stringr)
df <- df |>
  mutate(VEHICLE_YEAR = ifelse(str_detect(VEHICLE_YEAR, "^before"),0,1))
df |> select(VEHICLE_YEAR) |> slice_head(n = 10)
```
We have now replaced this data with 0s and 1s.

We have now used regular expression and string functions to clean our data. Now lets convert appropriate columns to factor variables.  The following columns need to be changed: `AGE`, `GENDER`, `RACE`, `DRIVING EXPERIENCE`, `EDUCATION`, `INCOME`, `VEHICLE_OWNERSHIP`, `VEHICLE YEAR`, `MARRIED`, `CHILDREN`, `VEHICHLE_TYPE`
```{r}
df <- df |>
  mutate(AGE = ifelse(AGE == 16, 0, ifelse(AGE == 26, 1, ifelse(AGE == 40, 2, 3)))) |>
  mutate(AGE = factor(AGE))

df <- df |>
  mutate(GENDER = ifelse(GENDER == "female", 0, 1)) |>
  mutate(GENDER = factor(GENDER))

df <- df |>
  mutate(RACE = ifelse(RACE == "minority", 0, 1)) |>
  mutate(RACE = factor(RACE))

df <- df |>
  mutate(DRIVING_EXPERIENCE = as.integer(str_extract(as.character(DRIVING_EXPERIENCE), "\\d"))) |>
  mutate(DRIVING_EXPERIENCE = factor(DRIVING_EXPERIENCE))


df <- df |>
  mutate(EDUCATION = ifelse(EDUCATION == "none", 0, ifelse(EDUCATION == "high school", 1, 2))) |>
  mutate(EDUCATION = factor(EDUCATION))

df <- df |>
  mutate(INCOME = ifelse(INCOME == "poverty", 0, ifelse(INCOME == "working class", 1, ifelse(INCOME == "middle class", 2, 3)))) |>
  mutate(INCOME = factor(INCOME))

df <- df |>
  mutate(VEHICLE_OWNERSHIP = factor(VEHICLE_OWNERSHIP))

df <- df |>
  mutate(VEHICLE_YEAR = factor(VEHICLE_YEAR))

df <- df |>
  mutate(MARRIED = factor(MARRIED))

df <- df |>
  mutate(CHILDREN = factor(CHILDREN))

df <- df |>
  mutate(ANNUAL_MILEAGE = factor(ANNUAL_MILEAGE))

df <- df |>
  mutate(VEHICLE_TYPE = ifelse(VEHICLE_TYPE == "sedan", 0, 1)) |>
  mutate(VEHICLE_TYPE = factor(VEHICLE_TYPE))

df <- df |>
  mutate(SPEEDING_VIOLATIONS = factor(SPEEDING_VIOLATIONS))

df <- df |>
  mutate(DUIS = factor(DUIS))

df <- df |>
  mutate(PAST_ACCIDENTS = factor(PAST_ACCIDENTS))
glimpse(df)
```
```{r}
write.csv(df, "factor_data.csv", row.names = FALSE)
```


```{r}
#df |>
  #count(AGE, name = "count") |>
  #arrange(desc(count))
```


# Data Exploration
Now that we have a clean dataset, we are ready to visualize our data to explore any trends and possibly verify any assumptions that we need for later on.  First however we are going to run a PCA to look at what variables are contributing the most to variance in the data.


## PCA
```{r}
df <- read.csv("factor_data.csv")
pca <- df |>
  select(!c(ID, OUTCOME)) |>
  prcomp(scale = TRUE)
```

Lets look at the principal component directions
```{r}
pca$rotation
```
This is hard to understand as there is simply too many numbers. We can use the `tidy` function to make this cleaner.
```{r}
library(broom)
rot_matrix <- pca |> tidy(matrix = "rotation")
pc1 <- rot_matrix |>
  filter(PC == 1) |>
  arrange(desc(abs(value)))

pc1
```
Prinicipal Component 1 is the direction with the most variance and the variables with the largest value are the largest contributors to this principal component direction.  We see that `AGE`, `INCOME`, and `DRIVING EXPERIENCE` are all very important to preserving variance in the data. Lets visualize the first 3 principal component directions and what variables are making up each direction.

```{r}
pca |>
  tidy(matrix = "loadings") |>
  filter(PC < 4) |>
  ggplot(aes (y = column, x = value)) +
  geom_col(aes(y = column)) + 
  labs(x = "Loadings", y = "Variable") +
  facet_wrap(~PC)
```
Here are some observations from the above graphic:
+ `VEHICHLE_YEAR`, `PAST_ACCIDENTS`, `INCOME`, `DRIVING_EXPERIENCE` seem to be important features
+ `VEHICHLE_TYPE`, `RACE`, `POSTAL CODE` do not seem to be important variables
+ `AGE` matters a lot in the first direction but not so much in the other directions
+ `ANNUAL_MILEAGE` is a massive contributor to principal component direction 3
+ `EDUCATION` and `INCOME` vary together indicating they are highly correlated.

Now we see how important each principal component direction is.

```{r}
pca |>
  tidy(matrix = "pcs") |>
  ggplot(aes(x = factor(PC), y = percent)) +
  geom_col() +
  labs(y = "Percent variance explained", x = "Principal Component")
```
Theres a lot of principal components so it may be easier to view this data in a table.
```{r}
pca |>
  tidy(matrix = "pcs") |>
  filter(cumulative < .90)
```
If we wanted to retain 90% of the variance in the data, we see that we would need the first 12 principal component directions.


## These are some super quick and messy models. I didn't do a train test split or anything. I just wwanted to get some graphics for the presentation...

```{r}
# 
df$OUTCOME <- as.factor(df$OUTCOME)
df[2:19] # This is all the predictors + outcome

```
```{r}
library(tidymodels)
library(tidyverse)
recipe_1 <- recipe(OUTCOME ~ .,
                   data = df[2:19])

recipe_1 <- recipe_1 |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize()

parsnip_1 <- logistic_reg(penalty = 0.0) |> 
  set_engine("glmnet") |> 
  set_mode("classification") 


workflow_1 <- workflow()

workflow_1 <- workflow_1 |>
  add_model(parsnip_1) |>
  add_recipe(recipe_1)

fit_1 <- workflow_1 |>
  fit(df[2:19])

preds_1 <- fit_1 |>
  predict(df[2:19], type = "class") |>
  pull(.pred_class)

confusion_mat = as.matrix(table(Actual_Values = df$OUTCOME, Predicted_Values = preds_1)) 
print(confusion_mat)

sum(preds_1 == df$OUTCOME) / length(df$OUTCOME)
```


```{r}




recipe_1 <- recipe(OUTCOME ~ .,
                   data = df[2:19])

recipe_1 <- recipe_1 |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize()

parsnip_2 <- rand_forest() |> 
  set_engine("ranger") |> 
  set_mode("classification")


workflow_2 <- workflow()

workflow_2 <- workflow_2 |>
  add_model(parsnip_2) |>
  add_recipe(recipe_1)

fit_2 <- workflow_2 |>
  fit(df[2:19])

preds_2 <- fit_2 |>
  predict(df[2:19], type = "class") |>
  pull(.pred_class)

confusion_mat = as.matrix(table(Actual_Values = df$OUTCOME, Predicted_Values = preds_2)) 
print(confusion_mat)

sum(preds_2 == df$OUTCOME) / length(df$OUTCOME)
```
```{r}
ggplot(df, aes(x = OUTCOME)) +
  geom_bar(fill = "steelblue") +
  labs(x = "OUTCOME", y = "Count", title = "Count of Rows for Each Class")
```

