---
title: "Report"
format: html
editor: visual
---

---
title: "STAT 5125 Final Project"
author: "Josh Moak and Owen Fiore"
date: "2024-04-22"
output: html_document
---

In this project we seek to explore patterns and identify trends in a dataset consisting of 10,000 car insurance claims available at [the link here](https://www.kaggle.com/datasets/sagnik1511/car-insurance-data). You can also find the data available in the repo at `Car_Insurance_Claim.csv`.

# Data Import

```{r}
df <- read.csv("Car_Insurance_Claim.csv")
df |> head()
```

We read in the data and we see that some data is character, some is numeric, and some is stored in a dbl.

```         
Regex: AGE, driving experience, vehicle year
```

# Data Cleaning

### Impute missing values

The first thing we are going to do is deal with missing data.

```{r}
library(tidyverse)
library(tidymodels)
library(visdat)
df |> vis_dat()
```

Here we see that we have a lot of data of type character which will likely need to be changed. However what we see is we only have 2 columns with missing data which is good.

```{r}
df |> vis_miss()
```

```{r}
library(naniar)
df |> miss_var_summary()
```

Lets impute missing values based on the median of the column

```{r}
library(naniar)
df <- df |>
  nabular(only_miss = TRUE) |>
  mutate(CREDIT_SCORE = naniar::impute_median(CREDIT_SCORE),
         ANNUAL_MILEAGE = naniar::impute_median(ANNUAL_MILEAGE)) |>
  select(-CREDIT_SCORE_NA, -ANNUAL_MILEAGE_NA)
```

```{r}
df |> n_miss()

```

### Regular Expression

We see there are no longer any missing values. We are now ready to use regular expression to alter and extract the meaningful string data. We have 3 columns of interest for this part: `AGE`, `Driving_Experince`, and `VEHICLE_YEAR`

```{r}
df |> select(AGE) |> slice_head(n = 10)
```

Using this and our knowledge of who drives cars, we know that all the information we need is going to be contained in the first two integers as somebody who is less than 10 years old cannot drive and anybody who would be 100 is grouped into 65+. Thus we need a regular expression to keep only the first 2 numbers.

```{r}
df <- df |>
  mutate(AGE = as.integer(stringr::str_extract(AGE, "^\\d{1,2}")))
df |> select(AGE) |> slice_head(n = 10)
```

This looks much better and is something that can be converted to a factor variable later on.

```{r}
df |> select(DRIVING_EXPERIENCE) |> slice_head(n = 10)
```

Lets use the `parse_number` function from the `readr` package to extract the first number available.

```{r}
df <- df |>
  mutate(DRIVING_EXPERIENCE = parse_number(DRIVING_EXPERIENCE))
df |> select(DRIVING_EXPERIENCE) |> slice_head(n = 10)
```

This looks much better.

Lastly we look at `VEHICLE_YEAR`.

```{r}
df |> select(VEHICLE_YEAR) |> slice_head(n = 10)
```

There seem to be only two types of data in this column: `before 2015` and `after 2015`. We are going to make this easier for us later on and convert this data to numeric.

```{r}
library(stringr)
df <- df |>
  mutate(VEHICLE_YEAR = ifelse(str_detect(VEHICLE_YEAR, "^before"),0,1))
df |> select(VEHICLE_YEAR) |> slice_head(n = 10)
```

We have now replaced this data with 0s and 1s.

We have now used regular expression and string functions to clean our data. Now lets convert appropriate columns to factor variables. The following columns need to be changed: `AGE`, `GENDER`, `RACE`, `DRIVING EXPERIENCE`, `EDUCATION`, `INCOME`, `VEHICLE_OWNERSHIP`, `VEHICLE YEAR`, `MARRIED`, `CHILDREN`, `VEHICHLE_TYPE`

```{r}
df <- df |>
  mutate(AGE = ifelse(AGE == 16, 0, ifelse(AGE == 26, 1, ifelse(AGE == 40, 2, 3)))) |>
  mutate(AGE = factor(AGE))

df <- df |>
  mutate(GENDER = ifelse(GENDER == "female", 0, 1)) |>
  mutate(GENDER = factor(GENDER))

df <- df |>
  mutate(RACE = ifelse(RACE == "minority", 0, 1)) |>
  mutate(RACE = factor(RACE))

df <- df |>
  mutate(DRIVING_EXPERIENCE = as.integer(str_extract(as.character(DRIVING_EXPERIENCE), "\\d"))) |>
  mutate(DRIVING_EXPERIENCE = factor(DRIVING_EXPERIENCE))


df <- df |>
  mutate(EDUCATION = ifelse(EDUCATION == "none", 0, ifelse(EDUCATION == "high school", 1, 2))) |>
  mutate(EDUCATION = factor(EDUCATION))

income_fct <- function(dataframe, col){
  dataframe <- dataframe |>
    mutate(col = )
}

df <- df |>
  mutate(INCOME = ifelse(INCOME == "poverty", 0, ifelse(INCOME == "working class", 1, ifelse(INCOME == "middle class", 2, 3)))) |>
  mutate(INCOME = factor(INCOME))

df <- df |>
  mutate(VEHICLE_OWNERSHIP = factor(VEHICLE_OWNERSHIP))

df <- df |>
  mutate(VEHICLE_YEAR = factor(VEHICLE_YEAR))

df <- df |>
  mutate(MARRIED = factor(MARRIED))

df <- df |>
  mutate(CHILDREN = factor(CHILDREN))

df <- df |>
  mutate(ANNUAL_MILEAGE = factor(ANNUAL_MILEAGE))

df <- df |>
  mutate(VEHICLE_TYPE = ifelse(VEHICLE_TYPE == "sedan", 0, 1)) |>
  mutate(VEHICLE_TYPE = factor(VEHICLE_TYPE))

df <- df |>
  mutate(SPEEDING_VIOLATIONS = factor(SPEEDING_VIOLATIONS))

df <- df |>
  mutate(DUIS = factor(DUIS))

df <- df |>
  mutate(PAST_ACCIDENTS = factor(PAST_ACCIDENTS))
glimpse(df)
```

```{r}
write.csv(df, "factor_data.csv", row.names = FALSE)
```

# Data Exploration

Now that we have a clean dataset, we are ready to visualize our data to explore any trends and possibly verify any assumptions that we need for later on. First however we are going to run a PCA to look at what variables are contributing the most to variance in the data.

## PCA

```{r}
df <- read.csv("factor_data.csv")
pca <- df |>
  select(!c(ID, OUTCOME)) |>
  prcomp(scale = TRUE)
```

Lets look at the principal component directions

```{r}
pca$rotation
```

This is hard to understand as there is simply too many numbers. We can use the `tidy` function to make this cleaner.

```{r}
library(broom)
rot_matrix <- pca |> tidy(matrix = "rotation")
pc1 <- rot_matrix |>
  filter(PC == 1) |>
  arrange(desc(abs(value)))

pc1
```

Prinicipal Component 1 is the direction with the most variance and the variables with the largest value are the largest contributors to this principal component direction. We see that `AGE`, `INCOME`, and `DRIVING EXPERIENCE` are all very important to preserving variance in the data. Lets visualize the first 3 principal component directions and what variables are making up each direction.

```{r}
pca |>
  tidy(matrix = "loadings") |>
  filter(PC < 4) |>
  ggplot(aes (y = column, x = value)) +
  geom_col(aes(y = column)) + 
  labs(x = "Loadings", y = "Variable") +
  facet_wrap(~PC)
```

Here are some observations from the above graphic: + `VEHICHLE_YEAR`, `PAST_ACCIDENTS`, `INCOME`, `DRIVING_EXPERIENCE` seem to be important features + `VEHICHLE_TYPE`, `RACE`, `POSTAL CODE` do not seem to be important variables + `AGE` matters a lot in the first direction but not so much in the other directions + `ANNUAL_MILEAGE` is a massive contributor to principal component direction 3 + `EDUCATION` and `INCOME` vary together indicating they are highly correlated.

Now we see how important each principal component direction is.

```{r}
pca |>
  tidy(matrix = "pcs") |>
  ggplot(aes(x = factor(PC), y = percent)) +
  geom_col() +
  labs(y = "Percent variance explained", x = "Principal Component")
```

Theres a lot of principal components so it may be easier to view this data in a table.

```{r}
pca |>
  tidy(matrix = "pcs") |>
  filter(cumulative < .90)
```

If we wanted to retain 90% of the variance in the data, we see that we would need the first 12 principal component directions.

## Exploratory Graphs

Lets look at the distribution of some of the variables to see if it is normal.

```{r}
df <- read.csv("factor_data.csv")

df |>
  ggplot(mapping = aes(x = CREDIT_SCORE)) +
  geom_histogram(bins = 20, fill = "khaki", color = "black")

#df["GENDER"] <- lapply(df["GENDER"], factor)
#df |>
  #ggplot(mapping = aes(x = CREDIT_SCORE, fill = GENDER)) +
  #geom_histogram(bins = 20)
```

We see that `CREDIT_SCORE` appears to be relatively normally distributed. Now lets see if there are any differences across `AGE`. The data is scaled, as credit scores typically range from 300 to 850.

```{r}
df["AGE"] <- lapply(df["AGE"], factor)
df |>
  ggplot(mapping = aes(x = CREDIT_SCORE, fill = AGE)) +
  geom_histogram(bins = 20)
```

AGE = 0 refers to the youngest and AGE = 3 refers to the oldest in the above graph. We see that older people have on average a higher credit score than the younger people, with very few people in the youngest age bracket having a credit score above 0.5.

```{r}
df |>
  ggplot(mapping = aes(x = PAST_ACCIDENTS, y = OUTCOME, color = GENDER)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  geom_jitter()
```

This plot contains three dimensions of data relating `OUTCOME`, `PAST_ACCIDENTS`, and `GENDER`. The pink dots correspond to women and the blue dots correspond to men. We see that there are a lot of men that have a lot of accidents. We also see that `PAST_ACCIDENTS` corresponds strongly with `OUTCOME` and that all observations that have more than 7 accidents are all did not file a claim. This may be due to the way the insurance company works with the company no longer accepting claims after a certain number of accidents.

```{r}
df |>
  ggplot(mapping = aes(x = EDUCATION, y = INCOME, color = CREDIT_SCORE)) +
  geom_jitter() +
  scale_color_gradient(low = "red", high = "green")
```

We visualize `INCOME`, `EDUCATION`, and `CREDIT_SCORE` and find see that there are very few individuals that have high education but low income or low education and high income, indicating that these are positively correlated with each other. Additionally, those with high education and low income have poor credit scores, likely because they have high debt. `CREDIT_SCORE` is difficult to visualize, but it seems that there are not significant changes going from left to right (`EDUCATION` AND `CREDIT_SCORE` are weakly correlated) but that points on the top are more green than those at the bottom indicating that `CREDIT_SCORE` and `INCOME` are positively correlated.

```{r}
df["AGE"] <- lapply(df["AGE"], factor)
df |>
  ggplot(mapping = aes(x = AGE, y = DRIVING_EXPERIENCE)) +
  geom_jitter()
```

As the density of the points along the diagonal are significantly higher than any other point on the graph, it is clear that `DRIVING_EXPERIENCE` and `AGE` are highly correlated with each other. It is also interesting that half this graph does not even exist because it is impossible to be in the youngest age and have lots of driving experience.

# Modeling

```{r}
df <- read.csv("factor_data.csv")
factor_cols <- c("AGE", "GENDER", "RACE", "DRIVING_EXPERIENCE",
                 "EDUCATION", "INCOME", "VEHICLE_OWNERSHIP",
                 "VEHICLE_YEAR", "MARRIED", "CHILDREN", "POSTAL_CODE",
                 "VEHICLE_TYPE", "SPEEDING_VIOLATIONS", "DUIS",
                 "PAST_ACCIDENTS", "OUTCOME")
df[factor_cols] <- lapply(df[factor_cols], factor)

df <- df[2:19] # This is all the predictors + outcome

```

Now that our data is clean and each column is the correct data type, let's create some sub samples of our data.

```{r}
set.seed(1)
ttsplit <- df |>
  initial_split(prop = 0.95)

ttsplit

train <- ttsplit |> training()
test <- ttsplit |> testing()
```

```{r}
table(train$OUTCOME)[1] / nrow(train)
table(test$OUTCOME)[1] / nrow(test)
```

The proportions within each split is roughly equal in terms of the `OUTCOME` variable.

## Model 1

First we try out a logstic regression model

```{r}
recipe_1 <- recipe(OUTCOME ~ .,
                   data = train)

recipe_1 <- recipe_1 |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize() 

parsnip_1 <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification") 


workflow_1 <- workflow()

workflow_1 <- workflow_1 |>
  add_model(parsnip_1) |>
  add_recipe(recipe_1)

fit_1 <- workflow_1 |>
  fit(train)

preds_1 <- fit_1 |>
  predict(test, type = "class") |>
  pull(.pred_class)

preds_1_prob <- fit_1 |>
  predict(test, type = "prob")

confusion_mat = as.matrix(table(Actual_Values = test$OUTCOME, Predicted_Values = preds_1)) 
print(confusion_mat)

```

## Model 2

Next we try a random forest model. We tried multiple models and picked these hyperparameters as a result.

```{r}
set.seed(1)
parsnip_2 <- rand_forest(mtry = 15, trees = 30) |> 
  set_engine("ranger") |> 
  set_mode("classification")


workflow_2 <- workflow()

workflow_2 <- workflow_2 |>
  add_model(parsnip_2) |>
  add_recipe(recipe_1)

fit_2 <- workflow_2 |>
  fit(train)

preds_2 <- fit_2 |>
  predict(test, type = "class") |>
  pull(.pred_class)

preds_2_prob <- fit_2 |>
  predict(test, type = "prob")

confusion_mat = as.matrix(table(Actual_Values = test$OUTCOME, Predicted_Values = preds_2)) 
print(confusion_mat)

```

## Model 3

Next we try a support vector machine.

```{r}
parsnip_3 <- svm_rbf() |>
  set_engine("kernlab") |>
  set_mode("classification")

workflow_3 <- workflow()

workflow_3 <- workflow_3 |>
  add_model(parsnip_3) |>
  add_recipe(recipe_1)

fit_3 <- workflow_3 |>
  fit(train)

preds_3 <- fit_3 |>
  predict(test, type = "class") |>
  pull(.pred_class)

preds_3_prob <- fit_3 |>
  predict(test, type = "prob")

confusion_mat = as.matrix(table(Actual_Values = test$OUTCOME, Predicted_Values = preds_3)) 
print(confusion_mat)


```

## Model 4

Next we try a boosted forest. We tried multiple hyperparameters and got strong performance with these ones.

```{r}
parsnip_4 <- boost_tree(trees = 10, learn_rate = 0.4,
                        tree_depth = 6) |>
  set_engine("xgboost") |>
  set_mode("classification")

workflow_4 <- workflow()

workflow_4 <- workflow_4 |>
  add_model(parsnip_4) |>
  add_recipe(recipe_1)

fit_4 <- workflow_4 |>
  fit(train)

preds_4 <- fit_4 |>
  predict(test, type = "class") |>
  pull(.pred_class)

preds_4_prob <- fit_4 |>
  predict(test, type = "prob")

confusion_mat = as.matrix(table(Actual_Values = test$OUTCOME, Predicted_Values = preds_4)) 
print(confusion_mat)

```

## Model 5

Our last model is a k nearest neighbors and we chose 17 after trying various other values.

```{r}
parsnip_5 <- nearest_neighbor(neighbors = 17) |>
  set_engine("kknn") |>
  set_mode("classification")

workflow_5 <- workflow()

workflow_5 <- workflow_5 |>
  add_model(parsnip_5) |>
  add_recipe(recipe_1)

fit_5 <- workflow_5 |>
  fit(train)

preds_5 <- fit_5 |>
  predict(test, type = "class") |>
  pull(.pred_class)

preds_5_prob <- fit_5 |>
  predict(test, type = "prob")

confusion_mat = as.matrix(table(Actual_Values = test$OUTCOME, Predicted_Values = preds_5)) 
print(confusion_mat)

```

We now have created five models and assessed how each of them did on the test data set. Let's store their values into a tibble so that we can easily visualize and analyze our results later. \## Model Assessment

```{r}
workflow_names <- c("glm", 
                    "rf",
                    "svm",
                    "xgboost",
                    "knn")

preds <- list(preds_1,
              preds_2,
              preds_3,
              preds_4,
              preds_5)

preds_prob <- list(preds_1_prob,
                   preds_2_prob,
                   preds_3_prob,
                   preds_4_prob,
                   preds_5_prob)

truth <- list(test$OUTCOME,
              test$OUTCOME,
              test$OUTCOME,
              test$OUTCOME,
              test$OUTCOME)

workflows_tbl <- tibble(work_names = workflow_names,
                        preds = preds,
                        preds_prob = preds_prob,
                        truth = truth)


```

Above we had already predicted how well each model did on test data. Now we evaluate how we did by comparing the ROC curves, AUC scores, and accuracy scores to select the best model.

```{r}
workflows_tbl |>
  unnest(cols = c(preds, preds_prob, truth)) |>
  group_by(work_names) |>
  roc_curve(truth = truth,
            .pred_1,
            event_level = "second") |>
  ggplot(aes(x = 1- specificity, 
             y = sensitivity, 
             color = work_names)) +
  geom_path()
```

```{r}
workflows_tbl |>
  unnest(cols = c(preds, preds_prob, truth)) |>
  group_by(work_names) |>
  roc_auc(truth = truth,
            .pred_1,
            event_level = "second") |>
  arrange(desc(.estimate)) |>
  ggplot(aes(x = .estimate,
             y = reorder(work_names, .estimate),
             fill = work_names)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(.estimate,3)), hjust = 1.1) + 
  labs(y = "Model",
       x = "AUC Score",
       fill = "Model")
  
```

```{r}
workflows_tbl |>
  group_by(work_names) |>
  unnest(cols = c(preds, truth)) |>
  mutate(acc = accuracy_vec(truth, preds)) |>
  nest(cols = c(preds,truth)) |>
  select(c(work_names, acc)) |>
  arrange(desc(acc)) |>
  ggplot(aes(x = acc,
             y = reorder(work_names, acc),
             fill = work_names)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(acc,3)), hjust = 1.1) + 
  labs(y = "Model",
       x = "Accuracy",
       fill = "Model")
```

## Model Selection

Out of the 5 models chosen, we examined two criteria: AUC score and accuracy. We measured each of these criterion on a testing/holdout subset of the original dataset. Each model was trained on a separate training subset. In last place for both measures was our knn model. Our XGboost model performed second best in each criterion, while the model with the highest AUC was the GLM, and the model with the highest accuracy was the SVM. Because the difference in each measure between first and second place is so marginal, we conclude that XGboost performed best.

```{r}
library(vip)
fit_4 |>
  extract_fit_parsnip() |> 
  vip(num_features = 15)
```

According to our preferred model, `VEHICLE_OWNERSHIP` and `DRIVING_EXPERIENCE` were the most important factors influencing `OUTCOME`.

## Conclusion

The five add ons that we chose were

-   PCA

-   Impute missing values

-   Clean data with regular expression

-    Make a github repo

To summarize, our goal was to predict whether or not a given customer would file an auto insurance claim. The insurance company has an interest in maximizing profits, so knowing which customers are more likely to file claims is quite valuable.

After cleaning the data, we were left with 18 predictors, most of which were categorical variables. Categorical variables can be difficult to visualize, so we tried our best using the jitter plots and looking at up to three dimensions of data before proceeding with modelling. On our chosen criteria, our XGBoost model performed the best with an accuracy of 85% on a holdout dataset. We then identified the features which were most important in the model's decision making process: `VEHICLE_OWNERSHIP` and `DRIVING_EXPERIENCE`. This seems pretty reasonable based on what we know about car insurance claims. If somebody does not own the car they are driving (They may rent it or lease it) they may not be likely to file a claim. They may need to fill out more paperwork with whatever company they rented the car from, but not necessarily file a claim. Additionally `DRIVING_EXPERIENCE` was an important feature, possibly because newer drivers may not want to file a claim and have their insurance premiums dramatically increase. If they think that they could afford to pay the damage out of pocket, they could choose to do so, and that drivers with much more experience may be more likely to file a claim.

To improve the quality of our predictions, we are interested in more detailed data. For example, the `AGE` column had 4 categories. Effectively, it lumped several age groups together. A more detailed dataset might reveal a difference between a 16 year old driver and a 24 year old driver, whereas our dataset was blind to such distinction. A history of each driver would also be a benefit of a more detailed dataset. Almost all the data being categorical dramatically reduced the effectiveness of it. When we started the project we didn't realize the data was structured like this until we started to dive into it and clean it. For instance the variable VECHICLE_YEAR was originally a binary variable with two options "before 2015" or "after 2015". If instead we had the precise year, this could have been a much more powerful variable that is predictive of OUTCOME.

## Next Steps

In the future we could try and find data relating to median income and education levels based on postal code and join it with future data sets. The postal code column by itself is not very helpful, but if we had been able to learn more about each type postal code and if there are any major highways or windy roads that could potentially lead to increased chance of an accident, perhaps interesting insights could be obtained. While we may need some domain knowledge of the postal codes available in the data in order to derive meaningful insights, joining the data set with another based on postal codes is still an interesting possibility to explore in the future.

While we did some informal hyperparameter tuning by testing a handful of parameters and then choosing the best one, it may be possible to get better results with even more tuning via grid search. However this can be computationally expensive and we were able to achieve satisfactory results using the approaches above.
